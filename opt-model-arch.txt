  : OPTForCausalLM(
  (model): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(50272, 2048, padding_idx=1)
      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)
      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-23): 24 x OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=2048, out_features=8192, bias=True)
          (fc2): Linear(in_features=8192, out_features=2048, bias=True)
          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)
)
  model: OPTModel(
  (decoder): OPTDecoder(
    (embed_tokens): Embedding(50272, 2048, padding_idx=1)
    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0-23): 24 x OPTDecoderLayer(
        (self_attn): OPTAttention(
          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
        )
        (activation_fn): ReLU()
        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=2048, out_features=8192, bias=True)
        (fc2): Linear(in_features=8192, out_features=2048, bias=True)
        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
  model.decoder: OPTDecoder(
  (embed_tokens): Embedding(50272, 2048, padding_idx=1)
  (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (layers): ModuleList(
    (0-23): 24 x OPTDecoderLayer(
      (self_attn): OPTAttention(
        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
        (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (activation_fn): ReLU()
      (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      (fc1): Linear(in_features=2048, out_features=8192, bias=True)
      (fc2): Linear(in_features=8192, out_features=2048, bias=True)
      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    )
  )
)
  model.decoder.embed_tokens: Embedding(50272, 2048, padding_idx=1)
  model.decoder.embed_positions: OPTLearnedPositionalEmbedding(2050, 2048)
  model.decoder.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers: ModuleList(
  (0-23): 24 x OPTDecoderLayer(
    (self_attn): OPTAttention(
      (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
      (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
    )
    (activation_fn): ReLU()
    (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (fc1): Linear(in_features=2048, out_features=8192, bias=True)
    (fc2): Linear(in_features=8192, out_features=2048, bias=True)
    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
)
  model.decoder.layers.0: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.0.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.0.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.0.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.0.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.0.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.0.activation_fn: ReLU()
  model.decoder.layers.0.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.0.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.0.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.0.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.1: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.1.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.1.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.1.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.1.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.1.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.1.activation_fn: ReLU()
  model.decoder.layers.1.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.1.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.1.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.1.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.2: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.2.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.2.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.2.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.2.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.2.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.2.activation_fn: ReLU()
  model.decoder.layers.2.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.2.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.2.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.2.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.3: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.3.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.3.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.3.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.3.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.3.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.3.activation_fn: ReLU()
  model.decoder.layers.3.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.3.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.3.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.3.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.4: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.4.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.4.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.4.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.4.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.4.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.4.activation_fn: ReLU()
  model.decoder.layers.4.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.4.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.4.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.4.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.5: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.5.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.5.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.5.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.5.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.5.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.5.activation_fn: ReLU()
  model.decoder.layers.5.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.5.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.5.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.5.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.6: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.6.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.6.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.6.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.6.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.6.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.6.activation_fn: ReLU()
  model.decoder.layers.6.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.6.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.6.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.6.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.7: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.7.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.7.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.7.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.7.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.7.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.7.activation_fn: ReLU()
  model.decoder.layers.7.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.7.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.7.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.7.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.8: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.8.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.8.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.8.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.8.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.8.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.8.activation_fn: ReLU()
  model.decoder.layers.8.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.8.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.8.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.8.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.9: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.9.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.9.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.9.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.9.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.9.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.9.activation_fn: ReLU()
  model.decoder.layers.9.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.9.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.9.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.9.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.10: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.10.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.10.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.10.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.10.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.10.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.10.activation_fn: ReLU()
  model.decoder.layers.10.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.10.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.10.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.10.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.11: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.11.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.11.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.11.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.11.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.11.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.11.activation_fn: ReLU()
  model.decoder.layers.11.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.11.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.11.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.11.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.12: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.12.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.12.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.12.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.12.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.12.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.12.activation_fn: ReLU()
  model.decoder.layers.12.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.12.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.12.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.12.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.13: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.13.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.13.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.13.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.13.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.13.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.13.activation_fn: ReLU()
  model.decoder.layers.13.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.13.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.13.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.13.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.14: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.14.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.14.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.14.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.14.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.14.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.14.activation_fn: ReLU()
  model.decoder.layers.14.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.14.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.14.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.14.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.15: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.15.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.15.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.15.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.15.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.15.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.15.activation_fn: ReLU()
  model.decoder.layers.15.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.15.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.15.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.15.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.16: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.16.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.16.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.16.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.16.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.16.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.16.activation_fn: ReLU()
  model.decoder.layers.16.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.16.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.16.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.16.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.17: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.17.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.17.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.17.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.17.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.17.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.17.activation_fn: ReLU()
  model.decoder.layers.17.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.17.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.17.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.17.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.18: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.18.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.18.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.18.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.18.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.18.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.18.activation_fn: ReLU()
  model.decoder.layers.18.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.18.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.18.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.18.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.19: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.19.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.19.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.19.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.19.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.19.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.19.activation_fn: ReLU()
  model.decoder.layers.19.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.19.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.19.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.19.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.20: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.20.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.20.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.20.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.20.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.20.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.20.activation_fn: ReLU()
  model.decoder.layers.20.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.20.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.20.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.20.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.21: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.21.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.21.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.21.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.21.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.21.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.21.activation_fn: ReLU()
  model.decoder.layers.21.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.21.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.21.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.21.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.22: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.22.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.22.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.22.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.22.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.22.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.22.activation_fn: ReLU()
  model.decoder.layers.22.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.22.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.22.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.22.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.23: OPTDecoderLayer(
  (self_attn): OPTAttention(
    (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
    (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (activation_fn): ReLU()
  (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  (fc1): Linear(in_features=2048, out_features=8192, bias=True)
  (fc2): Linear(in_features=8192, out_features=2048, bias=True)
  (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
)
  model.decoder.layers.23.self_attn: OPTAttention(
  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)
)
  model.decoder.layers.23.self_attn.k_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.23.self_attn.v_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.23.self_attn.q_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.23.self_attn.out_proj: Linear(in_features=2048, out_features=2048, bias=True)
  model.decoder.layers.23.activation_fn: ReLU()
  model.decoder.layers.23.self_attn_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  model.decoder.layers.23.fc1: Linear(in_features=2048, out_features=8192, bias=True)
  model.decoder.layers.23.fc2: Linear(in_features=8192, out_features=2048, bias=True)
  model.decoder.layers.23.final_layer_norm: LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  lm_head: Linear(in_features=2048, out_features=50272, bias=False)
model size= 5.26303232 GB
used time: 3.9957430362701416 seconds
</s>Hugging Face is pushing the convention that a unicorn with two horns becomes a llama.
He's a lama with horns. He's
{'': 0}
