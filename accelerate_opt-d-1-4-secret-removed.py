# -*- coding: utf-8 -*-
"""Accelerate_OPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7

# Running OPT up to 30B using `accelerate`

This notebook shows how to leverage the dispatching utility in colab, to load even very large checkpoints.

This should handle up to 11B in Colab Free, and 30B in colab Pro.
"""



"""This downloads the checkpoint. Several checkpoints are available:

- [facebook/opt-125m](https://huggingface.co/facebook/opt-125m)
- [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)
- [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)
- [facebook/opt-2.7b](https://huggingface.co/facebook/opt-2.7b)
- [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)
- [facebook/opt-13b](https://huggingface.co/facebook/opt-13b)
- [facebook/opt-30b](https://huggingface.co/facebook/opt-30b)
- [facebook/opt-66b](https://huggingface.co/facebook/opt-66b)

It downloads it to cache and we save the link to be re-used afterwards,

We then instantiate the model with an automatic model map, that will be created according to the current system's configuration.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
 
from accelerate import infer_auto_device_map

import time

hf_token = "hf"
my_model = 'facebook/opt-30b' 

t0 = time.time()
# ! mkdir offload_folder

#quantization_config = BitsAndBytesConfig(load_in_8bit=True )

#my_device_map = infer_auto_device_map(my_model, max_memory={0: "0GiB", "cpu":"10GiB"}, no_split_module_classes=["GPT2Block"])

model = AutoModelForCausalLM.from_pretrained(
    my_model, 
    device_map="auto", 
    offload_folder='./swap', 
    max_memory={0: "5GiB", "cpu":"20GiB"},
    token=hf_token , 
    #quantization_config= quantization_config ,
    )


 
print(f"model size= {model.get_memory_footprint()/1000000000} GB")

"""Finally, we create a prompt to generate from and we generate a text from it."""

tokenizer = AutoTokenizer.from_pretrained(my_model, use_fast=False, token=hf_token)
inputs = tokenizer("Hugging Face is pushing the convention that a unicorn with two horns becomes a llama.", return_tensors="pt")
  
output = model.generate(inputs["input_ids"].to(0), min_length=30, max_length=30, do_sample=True)
#output = model.generate(inputs["input_ids"].to('cpu'), min_length=30, max_length=30, do_sample=True)  # because model are offload to CPU
print(f"used time: {(time.time()-t0)} seconds")
    
print(tokenizer.decode(output[0].tolist()))

print(model.hf_device_map)



''' output 
 

'''
