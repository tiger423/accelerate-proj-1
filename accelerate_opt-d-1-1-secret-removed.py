# -*- coding: utf-8 -*-
"""Accelerate_OPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wnxMvD9zsiBQo2FtTpxn6w2cpXCcb-7

# Running OPT up to 30B using `accelerate`

This notebook shows how to leverage the dispatching utility in colab, to load even very large checkpoints.

This should handle up to 11B in Colab Free, and 30B in colab Pro.
"""



"""This downloads the checkpoint. Several checkpoints are available:

- [facebook/opt-125m](https://huggingface.co/facebook/opt-125m)
- [facebook/opt-350m](https://huggingface.co/facebook/opt-350m)
- [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)
- [facebook/opt-2.7b](https://huggingface.co/facebook/opt-2.7b)
- [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)
- [facebook/opt-13b](https://huggingface.co/facebook/opt-13b)
- [facebook/opt-30b](https://huggingface.co/facebook/opt-30b)
- [facebook/opt-66b](https://huggingface.co/facebook/opt-66b)

It downloads it to cache and we save the link to be re-used afterwards,

We then instantiate the model with an automatic model map, that will be created according to the current system's configuration.
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
 
from accelerate import infer_auto_device_map

import time

hf_token = "hf"
my_model = 'facebook/opt-30b' 

t0 = time.time()
# ! mkdir offload_folder

#quantization_config = BitsAndBytesConfig(load_in_8bit=True )

#my_device_map = infer_auto_device_map(my_model, max_memory={0: "0GiB", "cpu":"10GiB"}, no_split_module_classes=["GPT2Block"])

model = AutoModelForCausalLM.from_pretrained(
    my_model, 
    device_map="auto", 
    offload_folder='./swap', 
    max_memory={0: "0GiB", "cpu":"20GiB"},
    token=hf_token , 
    #quantization_config= quantization_config ,
    )


 
print(f"model size= {model.get_memory_footprint()/1000000000} GB")

"""Finally, we create a prompt to generate from and we generate a text from it."""

tokenizer = AutoTokenizer.from_pretrained(my_model, use_fast=False, token=hf_token)
inputs = tokenizer("Hugging Face is pushing the convention that a unicorn with two horns becomes a llama.", return_tensors="pt")
  
output = model.generate(inputs["input_ids"].to(0), min_length=30, max_length=30, do_sample=True)
#output = model.generate(inputs["input_ids"].to('cpu'), min_length=30, max_length=30, do_sample=True)  # because model are offload to CPU
print(f"used time: {(time.time()-t0)} seconds")
    
print(tokenizer.decode(output[0].tolist()))

print(model.hf_device_map)

'''
(py312) ve@ubuntu-105:~/llm/zero$ python accelerate_opt-d-1-1.py
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:09<00:00,  9.92s/it]
Some parameters are on the meta device because they were offloaded to the cpu and disk.
model size= 119.898161152 GB
/home/ve/anaconda3/envs/py312/lib/python3.12/site-packages/transformers/generation/utils.py:2412: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.
  warnings.warn(
used time: 362.2751078605652 seconds
</s>Hugging Face is pushing the convention that a unicorn with two horns becomes a llama. He knows. He's trying to trick us.

{'model.decoder.embed_tokens': 'cpu', 'lm_head': 'cpu', 'model.decoder.embed_positions': 'cpu', 'model.decoder.final_layer_norm': 'cpu', 'model.decoder.layers.0': 'cpu', 'model.decoder.layers.1': 'cpu', 'model.decoder.layers.2': 'cpu', 'model.decoder.layers.3': 'cpu', 'model.decoder.layers.4': 'cpu', 'model.decoder.layers.5': 'cpu', 'model.decoder.layers.6': 'cpu', 'model.decoder.layers.7': 'disk', 'model.decoder.layers.8': 'disk', 'model.decoder.layers.9': 'disk', 'model.decoder.layers.10': 'disk', 'model.decoder.layers.11': 'disk', 'model.decoder.layers.12': 'disk', 'model.decoder.layers.13': 'disk', 'model.decoder.layers.14': 'disk', 'model.decoder.layers.15': 'disk', 'model.decoder.layers.16': 'disk', 'model.decoder.layers.17': 'disk', 'model.decoder.layers.18': 'disk', 'model.decoder.layers.19': 'disk', 'model.decoder.layers.20': 'disk', 'model.decoder.layers.21': 'disk', 'model.decoder.layers.22': 'disk', 'model.decoder.layers.23': 'disk', 'model.decoder.layers.24': 'disk', 'model.decoder.layers.25': 'disk', 'model.decoder.layers.26': 'disk', 'model.decoder.layers.27': 'disk', 'model.decoder.layers.28': 'disk', 'model.decoder.layers.29': 'disk', 'model.decoder.layers.30': 'disk', 'model.decoder.layers.31': 'disk', 'model.decoder.layers.32': 'disk', 'model.decoder.layers.33': 'disk', 'model.decoder.layers.34': 'disk', 'model.decoder.layers.35': 'disk', 'model.decoder.layers.36': 'disk', 'model.decoder.layers.37': 'disk', 'model.decoder.layers.38': 'disk', 'model.decoder.layers.39': 'disk', 'model.decoder.layers.40': 'disk', 'model.decoder.layers.41': 'disk', 'model.decoder.layers.42': 'disk', 'model.decoder.layers.43': 'disk', 'model.decoder.layers.44': 'disk', 'model.decoder.layers.45': 'disk', 'model.decoder.layers.46': 'disk', 'model.decoder.layers.47': 'disk'}
'''
